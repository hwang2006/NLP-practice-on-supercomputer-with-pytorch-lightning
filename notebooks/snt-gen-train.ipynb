{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9962eab4-e4b2-479e-926d-cc52f982d182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-10 14:50:04.520767: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-10 14:50:04.674215: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-06-10 14:50:05.394153: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvrtc.so.11.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/cuda/11.7/lib64:/scratch/qualis/miniconda3/lib:/scratch/qualis/mecab/lib:/scratch/qualis/TensorRT-7.2.3.4/lib::\n",
      "2023-06-10 14:50:05.394629: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvrtc.so.11.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /apps/cuda/11.7/lib64:/scratch/qualis/miniconda3/lib:/scratch/qualis/mecab/lib:/scratch/qualis/TensorRT-7.2.3.4/lib::\n",
      "2023-06-10 14:50:05.394641: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ratsnlp.nlpbook.generation import GenerationTrainArguments\n",
    "args = GenerationTrainArguments(\n",
    "    pretrained_model_name=\"skt/kogpt2-base-v2\",\n",
    "    #pretrained_model_name=\"skt/ko-gpt-trinity-1.2B-v0.5\",\n",
    "    downstream_corpus_name=\"nsmc\",\n",
    "    downstream_model_dir=\"/scratch/qualis/nlp/checkpoint-generation\",\n",
    "    downstream_corpus_root_dir=\"/scratch/qualis/nlp\",\n",
    "    max_seq_length=32,\n",
    "    batch_size=32 if torch.cuda.is_available() else 4,\n",
    "    learning_rate=5e-5,\n",
    "    epochs=3,\n",
    "    tpu_cores=0 if torch.cuda.is_available() else 8,\n",
    "    seed=7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a39f182-9fa7-45a5-a6c3-2a441ac5f3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set seed: 7\n"
     ]
    }
   ],
   "source": [
    "from ratsnlp import nlpbook\n",
    "nlpbook.set_seed(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9071eb02-ff26-4110-9088-86f3c06cb890",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ratsnlp:Training/evaluation parameters GenerationTrainArguments(pretrained_model_name='skt/kogpt2-base-v2', downstream_task_name='sentence-generation', downstream_corpus_name='nsmc', downstream_corpus_root_dir='/scratch/qualis/nlp', downstream_model_dir='/scratch/qualis/nlp/checkpoint-generation', max_seq_length=32, save_top_k=1, monitor='min val_loss', seed=7, overwrite_cache=False, force_download=False, test_mode=False, learning_rate=5e-05, epochs=3, batch_size=32, cpu_workers=64, fp16=False, tpu_cores=0)\n"
     ]
    }
   ],
   "source": [
    "nlpbook.set_logger(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b047646b-302f-43cc-b3ff-687d40abcdfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Korpora] Corpus `nsmc` is already installed at /scratch/qualis/nlp/nsmc/ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at /scratch/qualis/nlp/nsmc/ratings_test.txt\n"
     ]
    }
   ],
   "source": [
    "from Korpora import Korpora\n",
    "Korpora.fetch(\n",
    "    corpus_name=args.downstream_corpus_name,\n",
    "    root_dir=args.downstream_corpus_root_dir,\n",
    "    force_download=args.force_download,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f189ebd7-ba76-4602-a7b5-c4cdde1baf90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "    args.pretrained_model_name,\n",
    "    eos_token=\"</s>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41c77d27-39ed-4171-9aea-3a1025f9d40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ratsnlp:Loading features from cached file /scratch/qualis/nlp/nsmc/cached_train_PreTrainedTokenizerFast_32_nsmc_sentence-generation [took 2.057 s]\n"
     ]
    }
   ],
   "source": [
    "from ratsnlp.nlpbook.generation import NsmcCorpus, GenerationDataset\n",
    "corpus = NsmcCorpus()\n",
    "train_dataset = GenerationDataset(\n",
    "    args=args,\n",
    "    corpus=corpus,\n",
    "    tokenizer=tokenizer,\n",
    "    mode=\"train\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f613ca8-4699-4c1e-b8a4-04a384d81225",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[GenerationFeatures(input_ids=[11775, 9050, 9267, 7700, 9705, 23971, 12870, 8262, 7055, 7098, 8084, 48213, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], labels=[11775, 9050, 9267, 7700, 9705, 23971, 12870, 8262, 7055, 7098, 8084, 48213, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
       " GenerationFeatures(input_ids=[16420, 19243, 29045, 8658, 11211, 11213, 9206, 7301, 14558, 8239, 10765, 8052, 7621, 31542, 15651, 20364, 8263, 9111, 12226, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], labels=[16420, 19243, 29045, 8658, 11211, 11213, 9206, 7301, 14558, 8239, 10765, 8052, 7621, 31542, 15651, 20364, 8263, 9111, 12226, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b40d880-ce4b-4d44-8c75-698dbe29e085",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/nlp-py310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    sampler=RandomSampler(train_dataset, replacement=False),\n",
    "    collate_fn=nlpbook.data_collator,\n",
    "    drop_last=False,\n",
    "    num_workers=args.cpu_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "577ab4ff-b6ae-4e9c-94bf-f567bd729cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:ratsnlp:Loading features from cached file /scratch/qualis/nlp/nsmc/cached_test_PreTrainedTokenizerFast_32_nsmc_sentence-generation [took 0.414 s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import SequentialSampler\n",
    "val_dataset = GenerationDataset(\n",
    "    args=args,\n",
    "    corpus=corpus,\n",
    "    tokenizer=tokenizer,\n",
    "    mode=\"test\",\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    sampler=SequentialSampler(val_dataset),\n",
    "    collate_fn=nlpbook.data_collator,\n",
    "    drop_last=False,\n",
    "    num_workers=args.cpu_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58885bb3-d734-4579-b428-df3bdeea6e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "model = GPT2LMHeadModel.from_pretrained(\n",
    "    args.pretrained_model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68e2deec-63c6-4dd9-a744-4c9ae34c852c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(51200, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=51200, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fa73888-31db-4800-86c7-cc02327eaa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ratsnlp.nlpbook.generation import GenerationTask\n",
    "task = GenerationTask(model, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7452ee11-ccc7-45ca-be01-151f084ddc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiprocessing is handled by SLURM.\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = nlpbook.get_trainer_org(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8de09a86-ee19-4c71-b303-7d71086532b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/miniconda3/envs/nlp-py310/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:608: UserWarning: Checkpoint directory /scratch/qualis/nlp/checkpoint-generation exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/scratch/qualis/miniconda3/envs/nlp-py310/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py:380: RuntimeWarning: Found unsupported keys in the optimizer configuration: {'scheduler'}\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name  | Type            | Params\n",
      "------------------------------------------\n",
      "0 | model | GPT2LMHeadModel | 125 M \n",
      "------------------------------------------\n",
      "125 M     Trainable params\n",
      "0         Non-trainable params\n",
      "125 M     Total params\n",
      "500.656   Total estimated model params size (MB)\n",
      "SLURM auto-requeueing enabled. Setting signal handlers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd518010ac0b4f9cae165938a19ff791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    task,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932850c7-5ee7-4173-91a7-4688558d9615",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28c22aba-8480-4ebb-b6ea-bfccc2fe2187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(51200, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=51200, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "model = GPT2LMHeadModel.from_pretrained(\n",
    "    \"skt/kogpt2-base-v2\",\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ee23196-9c26-4266-84f0-c64658cc9d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "    \"skt/kogpt2-base-v2\",\n",
    "    eos_token=\"</s>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1237a8c-2e39-4413-8d15-389cd8302120",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(\"안녕하세요\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea122316-f3f5-41b1-a062-2cc8ea3478c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[25906,  8702,  7801,  8084]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b145c5-95c2-4fac-a33f-c7e572182144",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3db72250-0472-4d24-89d9-b135c2259966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요?\"\n",
      "\"그럼, 그건 뭐예요?\"\n",
      "\"그럼, 그건 뭐예요?\"\n",
      "\"그럼, 그건 뭐예요?\"\n",
      "\"그럼, 그건 뭐예요?\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=False,\n",
    "        min_length=10,\n",
    "        max_length=50,\n",
    "    )\n",
    "    \n",
    "print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c390b53-68ff-4e20-a91d-7564a2da4400",
   "metadata": {},
   "source": [
    "## Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aecf01ca-357d-46d9-80a2-63c4c9dafc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요?\"\n",
      "\"그렇지 않습니다.\"\n",
      "\"그렇지 않습니다.\"\n",
      "\"그렇지 않습니다.\"\n",
      "\"그렇지 않습니다.\"\n",
      "\"그렇지 않습니다.\"\n",
      "\"그렇지 않습니다.\"\n",
      "\"그\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=False,\n",
    "        min_length=10,\n",
    "        max_length=50,\n",
    "        num_beams=3,\n",
    "    )\n",
    "    print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b13a42-adcb-4c1f-b6bc-1b22c2683ab6",
   "metadata": {},
   "source": [
    "## No repeation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36d173bb-043b-44be-982b-9435e3bbc229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요?\"\n",
      "\"그럼, 그건 뭐예요?\" 하고 나는 물었다.\n",
      "\"그건 뭐죠?\" 나는 물었다.\n",
      "나는 대답하지 않았다.\n",
      "\"그런데 왜 그걸 물어요? 그건 무슨 뜻이에요?\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=False,\n",
    "        min_length=10,\n",
    "        max_length=50,\n",
    "        no_repeat_ngram_size=3,\n",
    "    )\n",
    "    print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e52f366b-ca69-4f91-8a8b-58938238ce22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요?\"\n",
      "\"그럼, 그건 뭐예요?\"\n",
      "\"그럼, 그건 뭐예요?\"\n",
      "\"그럼, 그건 뭐예요?\"\n",
      "\"그럼, 그건 뭐예요?\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=False,\n",
    "        min_length=10,\n",
    "        max_length=50,\n",
    "        repetition_penalty=1.0,\n",
    "    )\n",
    "    print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1855c47-d20c-4c7c-8acf-1f7ac3fcb07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요?\"\n",
      "\"그럼, 그건 뭐예요?\"\n",
      "\"아니요, 저는요.\"\n",
      "\"그럼, 그건 무슨 말씀이신지요?\"\n",
      "\"그럼, 그건 뭐예요?\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=False,\n",
    "        min_length=10,\n",
    "        max_length=50,\n",
    "        repetition_penalty=1.1,\n",
    "    )\n",
    "    print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd917202-b977-4e04-b64b-7dbc084b79b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요?\"\n",
      "\"그럼, 그건 뭐예요, 아저씨. 저는 지금 이 순간에도 괜찮아요.\"\n",
      "\"그래서 오늘은 제가 할 수 있는 일이 무엇인지 말해 보겠습니다.\"\n",
      "\"이제\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=False,\n",
    "        min_length=10,\n",
    "        max_length=50,\n",
    "        repetition_penalty=1.2,\n",
    "    )\n",
    "    print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ffc913e7-7334-4b59-a5f3-05cb614d1783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요?\"\n",
      "\"그럼, 그건 뭐예요, 아저씨. 저는 지금 이 순간에도 괜찮아요. 그리고 제가 할 수 있는 일은 아무것도 없어요.\n",
      "이제 그만 돌아가고 싶어요.\n",
      "제가 하는 일이 무엇\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=False,\n",
    "        min_length=10,\n",
    "        max_length=50,\n",
    "        repetition_penalty=1.5,\n",
    "    )\n",
    "    print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112f8a32-0d50-47c4-beb9-31d61afa4756",
   "metadata": {},
   "source": [
    "# Top K Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d2f7789-3081-4bc7-a633-94a699717ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요!???????\n",
      "그럼, 당신이 이번에 다시 한번만 더 좋은 소식이 될 것 같네요...\n",
      "오늘 저녁 6시까지 오셨어요!!~\n",
      "오늘은 아침부터 오늘\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True,\n",
    "        min_length=10,\n",
    "        max_length=50,\n",
    "        top_k=50,\n",
    "    )\n",
    "    print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ecb7965-6dae-44e4-8d6e-7046ecfbd45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요?!!\n",
      "예~~ 예. 제발! 그래요..\n",
      "예. 근데 또 이게 이쁘지 않나요?!?!\n",
      "여튼 지금 뭔가 열심히 하는데요?!\n",
      "어쨌건!\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True,\n",
    "        min_length=10,\n",
    "        max_length=50,\n",
    "        top_k=50,\n",
    "    )\n",
    "    print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "145fb868-05b8-4a99-90b1-3cf417d15ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요?\"\n",
      "\"그럼, 그건 뭐예요?\"\n",
      "\"그럼, 그건 뭐예요?\"\n",
      "\"그럼, 그건 뭐예요?\"\n",
      "\"그럼, 그건 뭐예요?\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True,\n",
    "        min_length=10,\n",
    "        max_length=50,\n",
    "        top_k=1,\n",
    "    )\n",
    "    print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b51483f-73ec-459c-bf2c-e3c9114e0393",
   "metadata": {},
   "source": [
    "## Temperature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e491920-be17-40ce-a582-d58f629a9b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요?\"\n",
      "\"그럼, 그건 뭐예요?\"\n",
      "\"그럼, 그건 뭐예요?\"\n",
      "\"그럼, 그건 뭐예요?\"\n",
      "\"그럼, 그건 뭐예요?\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True,\n",
    "        min_length=10,\n",
    "        max_length=50,\n",
    "        top_k=50,\n",
    "        temperature=0.01,\n",
    "    )\n",
    "    print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23833bbd-90f4-474b-bde9-6f949f91bbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요라고 하면 아플 일이죠.\n",
      "자 제가 이렇게 궁금해지는 쵸 네 사실 네 그 요즘 너무 막 제가 안 해 볼 생각이 돼 가지신 아 쩜 안 좋다가 요러지고 어르케 네 한 이백이백 \n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True,\n",
    "        min_length=10,\n",
    "        max_length=50,\n",
    "        top_k=50,\n",
    "        temperature=100000000.0,\n",
    "    )\n",
    "    print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29acf533-fb12-4556-a807-220e44e2869c",
   "metadata": {},
   "source": [
    "## Top P Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "529f8802-0624-40e1-b96c-405461f7301c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요?\"\n",
      "\"네! 저는 이렇게 뵙겠습니다.\"\n",
      "\"그래서 이렇게 하죠.\"\n",
      "오가는 부인에게 말을 걸었다.\n",
      "\"내가 이렇게 해주었으니 얼마나 다행인가. 정말.\"\n",
      "\"그렇다면 우리도 기꺼이\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True,\n",
    "        min_length=10,\n",
    "        max_length=50,\n",
    "        top_p=0.92,\n",
    "    )\n",
    "    print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f82102e1-60ca-41d7-a087-e42a0cb3fda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요?\"\n",
      "\"내가 너한테 그걸 어떻게 하냐고 묻겠냐?\"\n",
      "\"아니에요, 네. 제가 저를 안아드리겠어요.\"\n",
      "갑자기 불쑥 묻는 불쑥 거절을 하면서 나에게는 다\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True,\n",
    "        min_length=10,\n",
    "        max_length=50,\n",
    "        top_p=0.92,\n",
    "    )\n",
    "    print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bbcd9a67-e86f-4d72-882e-f583b508625a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요?\"\n",
      "\"그럼, 그건 뭐예요?\"\n",
      "\"그럼, 그건 뭐예요?\"\n",
      "\"그럼, 그건 뭐예요?\"\n",
      "\"그럼, 그건 뭐예요?\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True,\n",
    "        min_length=10,\n",
    "        max_length=50,\n",
    "        top_p=0.01,\n",
    "    )\n",
    "    print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203c22e3-5c35-4909-9eee-ff1b1a03a367",
   "metadata": {},
   "source": [
    "## Overall parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f584125-ef6a-4890-bdc1-96b4a9f87201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요’하고 답했다.\n",
      "김현수 앵커는 “이번 회동에는 문재인 대통령이 ‘국민께 드리는 말씀’을 통해 취임 후 첫 메시지로 강조한 국민 통합과 한반도 평화정착의 큰 전환점이 되기를 기대한다”면서\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True,\n",
    "        min_length=10,\n",
    "        max_length=50,\n",
    "        repetition_penalty=1.5,\n",
    "        no_repeat_ngram_size=3,\n",
    "        temperature=0.9,\n",
    "        top_k=50,\n",
    "        top_p=0.92,\n",
    "    )\n",
    "    print(tokenizer.decode([el.item() for el in generated_ids[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9b9911-4fba-49f9-9cdb-6dce8b121bb0",
   "metadata": {},
   "source": [
    "# Model Testing 2 with the fine-tuned model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4210841e-754a-44a1-976b-698081190aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downstream_model_checkpoint_fpath: /scratch/qualis/nlp/checkpoint-generation/epoch=1-val_loss=2.29.ckpt\n"
     ]
    }
   ],
   "source": [
    "from ratsnlp.nlpbook.generation import GenerationDeployArguments\n",
    "args = GenerationDeployArguments(\n",
    "    pretrained_model_name=\"skt/kogpt2-base-v2\",\n",
    "    #pretrained_model_name=\"skt/ko-gpt-trinity-1.2B-v0.5\",\n",
    "    downstream_model_dir=\"/scratch/qualis/nlp/checkpoint-generation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ac2c184-c07d-4ff2-a883-f346962ff906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/qualis/nlp/checkpoint-generation/epoch=1-val_loss=2.29.ckpt\n"
     ]
    }
   ],
   "source": [
    "print(args.downstream_model_checkpoint_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2717bb25-8d31-4bc8-8cf5-b80c53a2a3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\n",
    "    args.pretrained_model_name,\n",
    "    eos_token=\"</s>\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "751cc6f9-581a-47b6-b799-ef9d635aa513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(51200, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=51200, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "#if args.downstream_model_checkpoint_path is None:\n",
    "if args.downstream_model_checkpoint_fpath is None:\n",
    "    model = GPT2LMHeadModel.from_pretrained(\n",
    "        args.pretrained_model_name,\n",
    "    )\n",
    "else:\n",
    "    #from google.colab import drive\n",
    "    #drive.mount('/gdrive', force_remount=True)    \n",
    "    pretrained_model_config = GPT2Config.from_pretrained(\n",
    "        args.pretrained_model_name,\n",
    "    )\n",
    "    model = GPT2LMHeadModel(pretrained_model_config)\n",
    "    fine_tuned_model_ckpt = torch.load(\n",
    "        #args.downstream_model_checkpoint_path,\n",
    "        args.downstream_model_checkpoint_fpath,\n",
    "        map_location=torch.device(\"cpu\"),\n",
    "    )\n",
    "    model.load_state_dict({k.replace(\"model.\", \"\"): v for k, v in fine_tuned_model_ckpt['state_dict'].items()})\n",
    "    #model.load_state_dict({k.replace(\"transformer.\", \"\"): v for k, v in fine_tuned_model_ckpt['state_dict'].items()})\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "695a1660-a5ee-4d7a-8aea-2f425a6d6a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_fn(\n",
    "        prompt,\n",
    "        min_length=10,\n",
    "        max_length=20,\n",
    "        top_p=1.0,\n",
    "        top_k=50,\n",
    "        repetition_penalty=1.0,\n",
    "        no_repeat_ngram_size=0,\n",
    "        temperature=1.0,\n",
    "):\n",
    "    try:\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                input_ids,\n",
    "                do_sample=True,\n",
    "                top_p=float(top_p),\n",
    "                top_k=int(top_k),\n",
    "                min_length=int(min_length),\n",
    "                max_length=int(max_length),\n",
    "                repetition_penalty=float(repetition_penalty),\n",
    "                no_repeat_ngram_size=int(no_repeat_ngram_size),\n",
    "                temperature=float(temperature),\n",
    "           )\n",
    "        generated_sentence = tokenizer.decode([el.item() for el in generated_ids[0]])\n",
    "    except:\n",
    "        generated_sentence = \"\"\"처리 중 오류가 발생했습니다. <br>\n",
    "            변수의 입력 범위를 확인하세요. <br><br> \n",
    "            min_length: 1 이상의 정수 <br>\n",
    "            max_length: 1 이상의 정수 <br>\n",
    "            top-p: 0 이상 1 이하의 실수 <br>\n",
    "            top-k: 1 이상의 정수 <br>\n",
    "            repetition_penalty: 1 이상의 실수 <br>\n",
    "            no_repeat_ngram_size: 1 이상의 정수 <br>\n",
    "            temperature: 0 이상의 실수\n",
    "            \"\"\"\n",
    "    return {\n",
    "        'result': generated_sentence,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ba9376e-facf-4809-abbd-763b68a72ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result': '긍정 아 정말 재밌게 재밋어요!</s>'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"긍정 아 정말\"\n",
    "inference_fn(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "76fae852-fbb8-47d4-a311-01ac789edec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result': '긍정 아 정말 재밌네요 ᄏᄏᄏ</s>'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"긍정 아 정말\"\n",
    "inference_fn(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "13ae65ea-d13d-4dc8-b5d9-36cc95323d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result': '긍정 아 정말 최고의 명장면이네...재밌었음..</s>'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"긍정 아 정말\"\n",
    "inference_fn(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8825b269-02d7-4a8c-abfd-718c4fd5b69a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result': '긍정 아 정말 잼있어ㅠㅠ 잘봤다</s>'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"긍정 아 정말\"\n",
    "inference_fn(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e1a5a3f7-e1be-4477-afb9-73d44312d080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result': '부정 아 정말 뭐 이렇게 유치하고 재미없다</s>'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"부정 아 정말\"\n",
    "inference_fn(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f971969d-9f92-4bd4-b558-7d74d6a9bae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result': '부정 아 정말 재미없다.. 이런영활 만드는것도 능력으로구나!</s>'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"부정 아 정말\"\n",
    "inference_fn(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dfe79d75-c1eb-46f3-b81d-85498e11b527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result': '부정 아 정말 너무 식상하고 뻔하다 내용도 뻔하고...</s>'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"부정 아 정말\"\n",
    "inference_fn(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da94a639-df2f-4e44-a008-340d13891fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-py310",
   "language": "python",
   "name": "nlp-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
